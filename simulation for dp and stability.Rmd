---
title: "FUSE"
author: "Matt Pelz"
date: "September 10, 2019"
output:
  pdf_document: default
  html_document: default
  word_document: default
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(BoolNet)
library(reshape2)
library(dplyr)
library(tidyr)
library(tibble)
library(data.table)
library(ggplot2)
library(purrr)
library(poisson)
library(poweRlaw)
library(actuar)
library(VGAM)
library(permute)
library(svglite)
```

network initiation
```{r}
# specify network size n
n <- 100

# specify connectivity k
avgK <- 4
k <- avgK

# numer of networks z
z <- 15

# number of initial conditions pairs
y <- 50

# specify topology distribution dist: 'uni','pl', or 'pois'
dist <- 'pois'

# specify function f: 'thresh50', 'thresh25', 'can2', or 'can1'
f <- 'thresh25'

# toggle determinative power reduction ('y') or random reduction ('n')
dpRedux <- 'y'
```

```{r}
# initiate 'summaryTable'which will store results vectors for each z
summaryTable <- data.frame(matrix(nrow = (n + 1)))
# the first column correspnds to the number of completed reductions
summaryTable[1] <- c(0:n)

# these dataframes will be used to generate derrida plots that include observations across all z generated networks
derrida100t <- data.frame(matrix(nrow = n, ncol = 1))
derrida100t1 <- data.frame(matrix(nrow = n, ncol = 1))
derrida75t <- data.frame(matrix(nrow = n, ncol = 1))
derrida75t1 <- data.frame(matrix(nrow = n, ncol = 1))
derrida50t <- data.frame(matrix(nrow = n, ncol = 1))
derrida50t1 <- data.frame(matrix(nrow = n, ncol = 1))
derrida25t <- data.frame(matrix(nrow = n, ncol = 1))
derrida25t1 <- data.frame(matrix(nrow = n, ncol = 1))

# the parentStability vector will store the average hamming ratio across all parent networks
parentStability <- vector()

# begin loop to run code z times
for (i in 1:z){

#generate distribution from network setting above
if(dist == 'uni'){
  k <- rdunif(n, 1, avgK*2)
} else if(dist == 'pois'){
  k <- rztpois(n, avgK)
} else if(dist == 'pl'){
  r <- 1.6
  s <- 11
  k <- rtruncpareto(n, 1, s, r) %>% round()
}
  
# generate network using 'BoolNet' R package. The wiring of the network will be tweaked and used. The structure generated by this command is a helpful starting point.
network <- generateRandomNKNetwork(n, k,topology = c("fixed"))

#store network 'interactions' in data table
interactions <- as.data.table(network$interactions)

#generate initial conditions randomly
ic1 <- data.frame(matrix(sample(0:1,n*y/2, replace=TRUE),n,y))

# generate s econd sert of initial conditions by switching 50 values in first column, 49 in second,..., 1 value in 50th column
ic2 <- ic1
for (i in 1:y){
  for (j in 1:i){
    ic2[i,j] <- abs(ic1[i,j] - 1)
  }
}

# initiate 'hammDists' and 'hammDistst1' which will store hamming distances after each reduction
hammDists <- data.frame(matrix(nrow = (n+1), ncol=(y)))
colnames(hammDists) <- c(1:y)
hammDistst1 <- data.frame(matrix(nrow = (n+1), ncol=(y)))
colnames(hammDistst1) <- c(1:y)

# measure hamming distances of initial conditions pairs for parent networks. Store in 1st row of 'hammDists'
for (i in 1:y){
  hammDists[1,i] = (sum(ic1[,i] != ic2[,i]))
}

# extract functions from 'interactions' as stacked vector of outputs '0' or '1'
functions <- as.matrix(interactions[2])[1,] %>% stack()

# extract inputs from 'interactions' as stacked vector of inputs between 1 and n
inputs <- as.matrix(interactions[1])[1,] %>% stack() %>% add_column(sequence(k), .before = 1)

# convert node labels to numeric vector and add vector to inputs data frame
temp <- gsub("Gene", "", inputs$ind) %>% as.numeric() %>% as.vector()
tempValues <- sample(temp, length(temp), replace = FALSE, prob = NULL)
tempDF <- cbind(temp, tempValues) %>% as.data.frame()
tempDF$dup <- duplicated(tempDF) %>% as.vector()
dups <- filter(tempDF, tempDF$dup == TRUE | tempDF$temp == 1 | tempDF$temp == 2)

#begin loop to remove duplicate input-output pairs
repeat{
    dups$tempValues <- sample(dups$tempValues, length(dups$tempValues),
    replace = FALSE, prob = NULL)
    dups <- dups[,1:2]
    tempDF <- tempDF[,1:2]
    tempDF <- filter(tempDF, tempDF$temp != 1)
    tempDF <- filter(tempDF, tempDF$temp != 2)
    tempFix <- rbind(unique(tempDF), dups)
  if(anyDuplicated(tempFix) == 0){
    break
  }
}

# develop structure for table of input-output relationships
tempFix <- tempFix[order(tempFix[,1]),]
inputs <- add_column(inputs, tempFix$temp, .before = 1)
inputs$values <- tempFix$tempValues
inputStore <- inputs

# create data frames with input values 0 and 1
inputZeroes <- data.frame(inputs, 0)
names(inputZeroes)[1]<-"node"
names(inputZeroes)[2]<-"inputIndex"
names(inputZeroes)[3]<-"inputNode"
names(inputZeroes)[5]<-"inputValue"
inputOnes <- data.frame(inputs, 1)
names(inputOnes)[1]<-"node"
names(inputOnes)[2]<-"inputIndex"
names(inputOnes)[3]<-"inputNode"
names(inputOnes)[5]<-"inputValue"

# merge inputZeroes and inputOnes to create new inputs. Clean up input-output table.
inputs <- rbind(inputZeroes, inputOnes)
inputs <- inputs[order(inputs[,1]),]
inputs <- inputs[(-4)]

#create structure of stacked truth tables
truthtables <- lapply(k, function(k) expand.grid(rep(list(0:1),times=k))) %>% plyr::rbind.fill()
truthtables <- add_column(truthtables, rep(1:n, 2^k), .before = 1)
names(truthtables)[1]<-"node"

## define function based on network setting specified above
# 50% threshold function: add all inputs, divide by two, add 0.5, and take floor value
# 25% threshold function: add all inputs, divide by two, add 0.5, and take floor value
# Canalyzing function depth 1
# Canalyzing function depth 2
if (f == 'thresh50'){
  truthtables$output <- rowMeans(truthtables[, -1], na.rm = TRUE)
  truthtables$output <- floor(truthtables$output + 0.5)
} else if (f == 'thresh25'){
  truthtables$output <- rowMeans(truthtables[, -1], na.rm = TRUE)
  truthtables$output <- floor(truthtables$output + 0.75)
} else if (f == 'can1'){
truthtables$output <- truthtables$Var1
numZeroes <- length(truthtables$output[truthtables$output==0])
truthtables$output[truthtables$output==0] <- rbinom(numZeroes, size = 1, prob=0.5)
} else if (f == 'can2'){
  truthtables$output <- pmax(truthtables$Var1, truthtables$Var2, na.rm=TRUE)
  numZeroes <- length(truthtables$output[truthtables$output==0])
  truthtables$output[truthtables$output==0] <- rbinom(numZeroes, size = 1, prob=0.5)
}

# calculate entropies for each node in a dataframe 'attributes'
# first, find the mean of all outputs values for each node. This equates to the probability that the output is 1.
attributes <- aggregate(truthtables$output, list(truthtables$node), mean)
colnames(attributes) <- c("node", "probOfOne")

# next, use formula for entropy to calculate entropy for each node: -(p(x=1)log2(p(x=1))-(p(x=0)log2(p(x=0))
attributes$nodeEntropy <- -(attributes$probOfOne)*log2(attributes$probOfOne)-((1-attributes$probOfOne)*log2(1-attributes$probOfOne))
attributes$nodeEntropy <- gsub("NaN", 0, attributes$nodeEntropy) %>% as.numeric()

# begin process of calculating DP values
# first, reorganize truthtables to allow for easier calculations
meltTables <- melt(truthtables, id=c("node", "output"))
meltTables$variable <- gsub("Var", "", meltTables$variable) %>% as.numeric()
names(meltTables)[3]<-"inputIndex"
names(meltTables)[4]<-"inputValue"
meltTables <- merge(meltTables, inputs, by=c("node", "inputIndex", "inputValue"))
meltTables <- meltTables[order(meltTables[,1]),]

# next, take mean outputs for each combination of nodes and inputs. this equates to the conditional probabilities of 1.
probTables <- aggregate(meltTables$output, list(meltTables$node, meltTables$inputValue, meltTables$inputIndex), mean)
colnames(probTables) <- c("node", "inputValue", "inputIndex", "probOutputOne")
probTables <- probTables[order(probTables[,1]),]

# next, add these values to the 'inputs' table and calculate the entropy for each condition. stored in column 'marginal entropy'
inputs <- merge(inputs, probTables)
inputs$marginalEntropy <- -(inputs$probOutputOne)*log2(inputs$probOutputOne)-((1-inputs$probOutputOne)*log2(1-inputs$probOutputOne))
inputs$marginalEntropy <- gsub("NaN", 0, inputs$marginalEntropy) %>% as.numeric()

# calculate conditional entropy for each combination of node and input by finding mean of the 'marginal entropy' values for each node-input combination.
mutualInf <- aggregate(inputs$marginalEntropy, list(inputs$node, inputs$inputIndex, inputs$inputNode), mean)
colnames(mutualInf) <- c("node", "inputIndex", "inputNode", "conditionalEntropy")
mutualInf <- attributes %>% select(c("node", "nodeEntropy")) %>% merge(mutualInf, by="node")
attributes$networkEntropy <- sum(mutualInf$conditionalEntropy)

# calculate mutual information by subtracting conditional entropy from node entropy for each node-input combination
mutualInf$mutualInformation <- mutualInf$nodeEntropy - mutualInf$conditionalEntropy    

# calculate dp and outlinks
# calculate DP by summing mutual information by input node
nodes <- data.frame(c(1:n))
colnames(nodes) <- c("node")
dp <- aggregate(mutualInf$mutualInformation, list(mutualInf$inputNode), sum)
colnames(dp) <- c("node", "dp")

# calculate outlinks by counting times a node appears in inputNode column. null values generated as NA, which we switch to 0.
ol <- as.data.frame(table(mutualInf$inputNode))
colnames(ol) <- c("node", "ol")
dp <- merge(nodes, dp, by="node", all=TRUE) %>% merge(ol, by ="node", all=TRUE)
dp[is.na(dp)] <- 0

# order nodes in 'attributes' by DP in ascending order
if (dpRedux == 'y'){
  attributes$determinativePower <- dp$dp
  attributes <- attributes[order(attributes[,5]),]
} else if (dpRedux == 'n'){
  attributes$randomOrder <- sample.int(n, n)
  attributes <- attributes[order(attributes[,5]),]
}

# add DP and outlinks values to 'attributes' dataframe
attributes$outlinks <- dp$ol

# add rank index. If arranged by DP values, node 1 has lowest DP
attributes$Rank <- c(1:n)

# iterate network to find H(t+1)
omega <- meltTables %>% select(1, 2, 3, 5) %>% unique()

# use initical conditions pairs as starting point
ic1t1 <- ic1
ic2t1 <- ic2
for (i in 1:y){
    ic <- ic1[i]
    ic$inputNode <- c(1:n)
    colnames(ic) <- c("inputValue", "inputNode")
    iterationTable <- merge(ic, omega)
    iterationTable <- iterationTable[order(iterationTable[,3]),]
    iterationTable <- iterationTable %>% select(1,3,4)
    iterationTable <- dcast(iterationTable,node ~ inputIndex, value.var = "inputValue")
    randomOutput <- rbinom(n, size = 1, prob=0.5)
    iterationTable <- add_column(iterationTable, randomOutput, .before = 1)
    
    icic <- ic2[i]
    icic$inputNode <- c(1:n)
    colnames(icic) <- c("inputValue", "inputNode")
    iterationTable2 <- merge(icic, omega)
    iterationTable2 <- iterationTable2[order(iterationTable2[,3]),]
    iterationTable2 <- iterationTable2 %>% select(1,3,4)
    iterationTable2 <- dcast(iterationTable2,node ~ inputIndex, value.var = "inputValue")
    iterationTable2 <- add_column(iterationTable2, randomOutput, .before = 1)
  
    if (f == 'thresh50'){
      iterationTable$output <- rowMeans(iterationTable[, -(1:2)], na.rm = TRUE)
      iterationTable$output <- floor(iterationTable$output + 0.5)
      iterationTable2$output <- rowMeans(iterationTable2[, -(1:2)], na.rm = TRUE)
      iterationTable2$output <- floor(iterationTable2$output + 0.5)
    } else if (f == 'thresh25'){
      iterationTable$output <- rowMeans(iterationTable[, -(1:2)], na.rm = TRUE)
      iterationTable$output <- floor(iterationTable$output + 0.75)
      iterationTable2$output <- rowMeans(iterationTable2[, -(1:2)], na.rm = TRUE)
      iterationTable2$output <- floor(iterationTable2$output + 0.75)
    } else if (f == 'can1'){
      iterationTable$output <- pmax(iterationTable[,1], iterationTable[,3], na.rm=TRUE)
      iterationTable2$output <- pmax(iterationTable2[,1], iterationTable2[,3], na.rm=TRUE)
    } else if (f == 'can2'){
      iterationTable$output <- pmax(iterationTable[,1], iterationTable[,3], iterationTable[,4], na.rm=TRUE)
      iterationTable2$output <- pmax(iterationTable2[,1], iterationTable2[,3], iterationTable2[,4], na.rm=TRUE)
    }
    
    ic1t1[i] <- iterationTable$output
    ic2t1[i] <- iterationTable2$output
  }

# measure hamming distances of initial conditions pairs for parent networks
for (i in 1:y){
  hammDistst1[1,i] = sum(ic1t1[,i] != ic2t1[,i])
}

# merge 'attributes' with 'meltTables' and order to facilitate network reduction process. Order resulting 'reductTable' dataframe by Rank
reductTable <- merge(meltTables, attributes, by="node") %>% select(11, 1, 9, 10, 6:7, 2:5, 8)
reductTable <- reductTable[order(reductTable[,1]),]

# initiate vector 'alpha' to store indices of removed nodes
alpha <- vector(mode="integer", length=n)

# add observations to derrida plot of parent network
derrida <- data.frame(unlist(hammDists[1,])/n, unlist(hammDistst1[1,])/n)
colnames(derrida) <- c("t", "t1")
derrida100t <- cbind(derrida100t, derrida$t)
derrida100t1 <- cbind(derrida100t1, derrida$t1)


# begin network reduction loop
# begin with node with dpRank j = 1 and proceed to n. 
for( j in 1:(n)){
  # isolate nodes with Rank = j
  ranks <- filter(reductTable, Rank == j)

  # store node number in vector 'alpha'
  alpha[j] <- ranks[1,2]

  # wherever an input node is 'alpha', set value to 0
  for (i in 1:n){
   if (i == alpha[j]){
    ic1[i,] = 0
    ic2[i,] = 0
   }
  }

  # wherever a node is equal to 'alpha', set output to 0
  reductTable <- within.data.frame(reductTable, output[node == alpha[j]] <- 0)

  # save hamming distances of ic pairs of subnetwork in next row of 'hammDists'
  for (i in 1:y){
    hammDists[j+1,i] = sum(ic1[,i] != ic2[,i])
  }

  # iterate network to find H(t+1)
  omega <- meltTables %>% select(1, 2, 3, 5) %>% unique()
  ic1t1 <- ic1
  ic2t1 <- ic2
  for (i in 1:y){
    ic <- ic1[i]
    ic$inputNode <- c(1:n)
    colnames(ic) <- c("inputValue", "inputNode")
    iterationTable <- merge(ic, omega)
    iterationTable <- iterationTable[order(iterationTable[,3]),]
    iterationTable <- iterationTable %>% select(1,3,4)
    iterationTable <- dcast(iterationTable,node ~ inputIndex, value.var = "inputValue")
    randomOutput <- rbinom(n, size = 1, prob=0.5)
    iterationTable <- add_column(iterationTable, randomOutput, .before = 1)
    
    icic <- ic2[i]
    icic$inputNode <- c(1:n)
    colnames(icic) <- c("inputValue", "inputNode")
    iterationTable2 <- merge(icic, omega)
    iterationTable2 <- iterationTable2[order(iterationTable2[,3]),]
    iterationTable2 <- iterationTable2 %>% select(1,3,4)
    iterationTable2 <- dcast(iterationTable2,node ~ inputIndex, value.var = "inputValue")
    iterationTable2 <- add_column(iterationTable2, randomOutput, .before = 1)
  
    if (f == 'thresh50'){
      iterationTable$output <- rowMeans(iterationTable[, -(1:2)], na.rm = TRUE)
      iterationTable$output <- floor(iterationTable$output + 0.5)
      iterationTable2$output <- rowMeans(iterationTable2[, -(1:2)], na.rm = TRUE)
      iterationTable2$output <- floor(iterationTable2$output + 0.5)
    } else if (f == 'thresh25'){
      iterationTable$output <- rowMeans(iterationTable[, -(1:2)], na.rm = TRUE)
      iterationTable$output <- floor(iterationTable$output + 0.75)
      iterationTable2$output <- rowMeans(iterationTable2[, -(1:2)], na.rm = TRUE)
      iterationTable2$output <- floor(iterationTable2$output + 0.75)
    } else if (f == 'can1'){
      iterationTable$output <- pmax(iterationTable[,1], iterationTable[,3], na.rm=TRUE)
      iterationTable2$output <- pmax(iterationTable2[,1], iterationTable2[,3], na.rm=TRUE)
    } else if (f == 'can2'){
      iterationTable$output <- pmax(iterationTable[,1], iterationTable[,3], iterationTable[,4], na.rm=TRUE)
      iterationTable2$output <- pmax(iterationTable2[,1], iterationTable2[,3], iterationTable2[,4],
      na.rm=TRUE)
    }
    
    t <- iterationTable$output
    u <- iterationTable2$output
    for (m in 1:n){
      s <- alpha[m]
      t[s] <- 0
      u[s] <- 0
    }
    iterationTable$output <- t
    iterationTable2$output <- u
    
    ic1t1[i] <- iterationTable$output
    ic2t1[i] <- iterationTable2$output
  }
  
  # measure and store hamming distances of t + 1 pairs for subnetworks
  for (i in 1:y){
    hammDistst1[j+1,i] = sum(ic1t1[,i] != ic2t1[,i])
  }
  
  
    if (j == 25){
      derrida75 <- data.frame(unlist(hammDists[j,]), unlist(hammDistst1[j,]))
      colnames(derrida75) <- c("t", "t1")
      derrida75$t <- derrida75$t/(n-j)
      derrida75$t1 <- derrida75$t1/(n-j)
      derrida75t <- cbind(derrida75t, derrida75$t)
      derrida75t1 <- cbind(derrida75t1, derrida75$t1)
    }
  
    if (j == 50){
      derrida50 <- data.frame(unlist(hammDists[j,]), unlist(hammDistst1[j,]))
      colnames(derrida50) <- c("t", "t1")
      derrida50$t <- derrida50$t/(n-j)
      derrida50$t1 <- derrida50$t1/(n-j)
      derrida50t <- cbind(derrida50t, derrida50$t)
      derrida50t1 <- cbind(derrida50t1, derrida50$t1)
    }

    if (j == 75){
      derrida25 <- data.frame(unlist(hammDists[j,]), unlist(hammDistst1[j,]))
      colnames(derrida25) <- c("t", "t1")
      derrida25$t <- derrida25$t/(n-j)
      derrida25$t1 <- derrida25$t1/(n-j)
      derrida25t <- cbind(derrida25t, derrida25$t)
      derrida25t1 <- cbind(derrida25t1, derrida25$t1)
    }
}
# end of reduction loop


# create vectors denoting reduction index. Divide hamming distances by reduction index
hammDists$size <- c(n:0)
hammDistst1$size <- c(n:0)

  
# for (i in 1:y){
#   hammDists[,i] <- hammDists[,i]/hammDists[,(y+1)]
# }
#   
# for (i in 1:y){
#   hammDistst1[,i] <- hammDistst1[,i]/hammDistst1[,(y+1)]
# }

hammDists[(n+1),] <- 0
hammDistst1[(n+1),] <- 0


#Find sub-to-parent ratios between h(t) and h(t+1) for each subnetwork
hammRats <- data.frame(matrix(nrow = (n+1), ncol=(y)))
for (i in 1:(n+1)){
  for (j in 1:y){
    hammRats[i,j] <- hammDistst1[i,j]/hammDists[i,j]
  }
}

# change '0' values in first row to NA to avoid dividing by 0
for (i in 1:y){
  if (hammRats[1,i] == 0){
    hammRats[1,i] <- NA
  }
}

temporary <- rowMeans(hammRats, na.rm = TRUE)
parentStability <- append(parentStability, temporary[1])
  
subToParentRats <- data.frame(matrix(nrow = (n+1), ncol=(y)))
for (i in 1:(n+1)){
  for (j in 1:y){
    subToParentRats[i,j] <- hammRats[i,j]/hammRats[1,j]
  }
}

summaryTable <- cbind(summaryTable, rowMeans(subToParentRats, na.rm = TRUE))

}

#only valid if z=10
derrida100t1 <- c(derrida100t1[, 2], derrida100t1[, 3], derrida100t1[, 4], derrida100t1[, 5], derrida100t1[, 6], derrida100t1[, 7], derrida100t1[, 8], derrida100t1[, 9], derrida100t1[, 10], derrida100t1[, 11], derrida100t1[, 12], derrida100t1[, 13], derrida100t1[, 14], derrida100t1[, 15], derrida100t1[, 16])
derrida100t <- c(derrida100t[, 2], derrida100t[, 3], derrida100t[, 4], derrida100t[, 5], derrida100t[, 6], derrida100t[, 7], derrida100t[, 8], derrida100t[, 9], derrida100t[, 10], derrida100t[, 11], derrida100t[, 12], derrida100t[, 13], derrida100t[, 14], derrida100t[, 15], derrida100t[, 16])
derrida100Full <- data.frame(derrida100t, derrida100t1)

derrida100FullPlot <- ggplot(derrida100Full, aes(x = derrida100t, y=derrida100t1)) + geom_point() + geom_smooth(se= FALSE) + labs(x = "H(t)", y = "H(t+1)") + geom_abline(slope = 1) + xlim(0, 0.5) + ylim(0, 0.5)
ggsave("derrida100FullPlot.png")
ggsave("derrida100FullPlot.svg")

derrida75t1 <- c(derrida75t1[, 2], derrida75t1[, 3], derrida75t1[, 4], derrida75t1[, 5], derrida75t1[, 6], derrida75t1[, 7], derrida75t1[, 8], derrida75t1[, 9], derrida75t1[, 10], derrida75t1[, 11], derrida75t1[, 12], derrida75t1[, 13], derrida75t1[, 14], derrida75t1[, 15], derrida75t1[, 16])
derrida75t <- c(derrida75t[, 2], derrida75t[, 3], derrida75t[, 4], derrida75t[, 5], derrida75t[, 6], derrida75t[, 7], derrida75t[, 8], derrida75t[, 9], derrida75t[, 10], derrida75t[, 11], derrida75t[, 12], derrida75t[, 13], derrida75t[, 14], derrida75t[, 15], derrida75t[, 16])
derrida75Full <- data.frame(derrida75t, derrida75t1)

derrida75FullPlot <- ggplot(derrida75Full, aes(x = derrida75t, y=derrida75t1)) + geom_point() + geom_smooth(se= FALSE) + labs(x = "H(t)", y = "H(t+1)") + geom_abline(slope = 1) + xlim(0, 0.5) + ylim(0, 0.5)
ggsave("derrida75FullPlot.png")
ggsave("derrida75FullPlot.svg")

derrida50t1 <- c(derrida50t1[, 2], derrida50t1[, 3], derrida50t1[, 4], derrida50t1[, 5], derrida50t1[, 6], derrida50t1[, 7], derrida50t1[, 8], derrida50t1[, 9], derrida50t1[, 10], derrida50t1[, 11], derrida50t1[, 12], derrida50t1[, 13], derrida50t1[, 14], derrida50t1[, 15], derrida50t1[, 16])
derrida50t <- c(derrida50t[, 2], derrida50t[, 3], derrida50t[, 4], derrida50t[, 5], derrida50t[, 6], derrida50t[, 7], derrida50t[, 8], derrida50t[, 9], derrida50t[, 10], derrida50t[, 11], derrida50t[, 12], derrida50t[, 13], derrida50t[, 14], derrida50t[, 15], derrida50t[, 16])
derrida50Full <- data.frame(derrida50t, derrida50t1)

derrida50FullPlot <- ggplot(derrida50Full, aes(x = derrida50t, y=derrida50t1)) + geom_point() + geom_smooth(se= FALSE) + labs(x = "H(t)", y = "H(t+1)") + geom_abline(slope = 1) + xlim(0, 0.5) + ylim(0, 0.5)
ggsave("derrida50FullPlot.png")
ggsave("derrida50FullPlot.svg")

derrida25t1 <- c(derrida25t1[, 2], derrida25t1[, 3], derrida25t1[, 4], derrida25t1[, 5], derrida25t1[, 6], derrida25t1[, 7], derrida25t1[, 8], derrida25t1[, 9], derrida25t1[, 10], derrida25t1[, 11], derrida25t1[, 12], derrida25t1[, 13], derrida25t1[, 14], derrida25t1[, 15], derrida25t1[, 16])
derrida25t <- c(derrida25t[, 2], derrida25t[, 3], derrida25t[, 4], derrida25t[, 5], derrida25t[, 6], derrida25t[, 7], derrida25t[, 8], derrida25t[, 9], derrida25t[, 10], derrida25t[, 11], derrida25t[, 12], derrida25t[, 13], derrida25t[, 14], derrida25t[, 15], derrida25t[, 16])
derrida25Full <- data.frame(derrida25t, derrida25t1)

derrida25FullPlot <- ggplot(derrida25Full, aes(x = derrida25t, y=derrida25t1)) + geom_point() + geom_smooth(se= FALSE) + labs(x = "H(t)", y = "H(t+1)") + geom_abline(slope = 1) + xlim(0, 0.5) + ylim(0, 0.5)
ggsave("derrida25FullPlot.png")
ggsave("derrida25FullPlot.svg")

summaryTable <- summaryTable[-1]
meanValues <- rowMeans(summaryTable)
curve <- cbind(c(0:n), meanValues) %>% as.data.frame()
colnames(curve) <- c("Reductions", "values")
entropyPlot <- ggplot(curve, aes(Reductions, values)) + geom_line() + labs(x = "Node Reductions", y = "Sub to Parent Ratio of Hamming Ratios")  + ylim(0, 1.75)
ggsave("hammingRatioPlot.png")
ggsave("hammingRatioPlot.svg")
write.csv(summaryTable, "summaryTable.csv")
write.csv(curve, "meansTable.csv")
write.csv(parentStability, "parentStability.csv")

```







```

